{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmentation_building.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshhpathak/DisasterAssessment/blob/master/segmentation_building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMUBqsU6n8OO",
        "colab_type": "code",
        "outputId": "d788aff7-7a51-4ca2-9de0-118475df7251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp5VK_kdr3n-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIYt5SGyoDvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Basic_blocks = drive.CreateFile({'id':'1LIJahCBOgKswPjV3dCdjno6iQunL1JQN'})\n",
        "#UNet = drive.CreateFile({'id':'1Xf0xSo-CswXiuiUR_tc9IbvzRx-UWU7M'})\n",
        "UNet = drive.CreateFile({'id':'1OcRzMCigngoFCev3M_WrkS_oCg1ZqBgp'})\n",
        "utils = drive.CreateFile({'id':'1VEQdnYI1HMy0mp8BtbdiGWz-BWSzDn5y'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKPeQn4du1qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Basic_blocks.GetContentFile('Basic_blocks.py')\n",
        "UNet.GetContentFile('UNet_param.py')\n",
        "utils.GetContentFile('utils.py')\n",
        "#UNet.GetContentFile('UNet.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk0_tJp-vp8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from UNet_param import *\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch.utils as utils\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import torchvision.utils as v_utils\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7-hyKcjvwX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters\n",
        "batch_size = 32\n",
        "img_size = 256\n",
        "lr = 0.00005\n",
        "epoch = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOKNY-LHJzxO",
        "colab_type": "code",
        "outputId": "05695603-3fec-4ed1-8921-90cb49510e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 908
        }
      },
      "source": [
        "img_dir = \"/content/drive/My Drive/Major Project/dataset/building/crowdAI/train\"\n",
        "\n",
        "img_data = dset.ImageFolder(root=img_dir, transform=transforms.Compose([\n",
        "    transforms.Resize(size=img_size),\n",
        "    transforms.CenterCrop(size=(img_size, img_size * 2)),\n",
        "    transforms.ToTensor(),\n",
        "]))\n",
        "img_batch = data.DataLoader(img_data, batch_size=batch_size,\n",
        "                            shuffle=True,pin_memory=True,num_workers=8)\n",
        "print(img_data)\n",
        "\n",
        "generator = UNet(pool_kernel=3, pool_stride=2,repeat_blocks=4, n_filters=16, out_channels=1).cuda()\n",
        "\n",
        "# load pretrained model\n",
        "\n",
        "path = \"/content/drive/My Drive/Major Project/code/building/crowdAI/model/unet.pkl\"\n",
        "\n",
        "try:\n",
        "    generator = torch.load(path)\n",
        "    print(\"\\n--------model restored--------\\n\")\n",
        "except:\n",
        "    print(\"\\n--------model not restored--------\\n\")\n",
        "    pass\n",
        "  \n",
        "  \n",
        "filepath = \"/content/drive/My Drive/Major Project/code/building/crowdAI/error/mse2\"\n",
        "\n",
        "# loss function \n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "# training\n",
        "\n",
        "file = open(filepath, 'w')\n",
        "for i in range(epoch):\n",
        "    for _, (image, label) in enumerate(img_batch):\n",
        "        satel_image, map_image = torch.chunk(image, chunks=2, dim=3)\n",
        "        map_image = map_image[:,0:1,:,:]\n",
        "        \n",
        "        gen_optimizer.zero_grad()\n",
        "\n",
        "        x = Variable(satel_image).cuda(0)\n",
        "        y_ = Variable(map_image).cuda(0)\n",
        "        y = generator.forward(x)\n",
        "\n",
        "        loss = loss_func(y, y_)\n",
        "        file.write(str(loss) + \"\\n\")\n",
        "        loss.backward()\n",
        "        gen_optimizer.step()\n",
        "           \n",
        "            \n",
        "        if _ % 500 == 0:\n",
        "            torch.save(generator, path)\n",
        "            print(i,loss)           \n",
        "            #original = \"/content/drive/My Drive/Major Project/code/building/crowdAI/result/filter_16/1/\"+\"original_image_{}_{}.png\".format(i, _)\n",
        "            #label = \"/content/drive/My Drive/Major Project/code/building/crowdAI/result/filter_16/1/\"+\"label_image_{}_{}.png\".format(i, _)\n",
        "            #gen = \"/content/drive/My Drive/Major Project/code/building/crowdAI/result/filter_16/1/\"+\"gen_image_{}_{}.png\".format(i, _)\n",
        "            #v_utils.save_image(x.cpu().data, original)\n",
        "            #v_utils.save_image(y_.cpu().data, label)\n",
        "            #v_utils.save_image(y.cpu().data, gen)\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset ImageFolder\n",
            "    Number of datapoints: 256633\n",
            "    Root Location: /content/drive/My Drive/Major Project/dataset/building/crowdAI/train\n",
            "    Transforms (if any): Compose(\n",
            "                             Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
            "                             CenterCrop(size=(256, 512))\n",
            "                             ToTensor()\n",
            "                         )\n",
            "    Target Transforms (if any): None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/UNet_param.py:22: UserWarning: Please make sure, that your input tensor's dimensions are divisible by (pool_stride ** repeat_blocks)\n",
            "  warnings.warn(\"Please make sure, that your input tensor's dimensions are divisible by \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--------model restored--------\n",
            "\n",
            "0 tensor(0.0192, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0231, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0253, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0193, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0196, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0219, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0184, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0183, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0204, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0212, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0170, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0157, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0244, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0183, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "0 tensor(0.0190, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0190, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0223, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0187, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0175, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0170, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0261, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0186, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0216, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0171, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0180, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0172, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0243, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0197, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0178, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0206, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.0169, device='cuda:0', grad_fn=<MseLossBackward>)\n",
            "2 tensor(0.0173, device='cuda:0', grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}